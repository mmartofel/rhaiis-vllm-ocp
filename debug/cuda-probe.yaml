  apiVersion: v1
  kind: Pod
  metadata:
    name: cuda-probe
    namespace: vllm-inference
  spec:
    restartPolicy: Never
    runtimeClassName: nvidia
    containers:
    - name: probe
      image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3
      command: ["/bin/sh", "-c"]
      args:
      - |
        echo "=== NVIDIA / CUDA env vars ==="
        env | grep -E 'NVIDIA|CUDA|LD_LIBRARY' | sort

        echo ""
        echo "=== libcuda.so locations ==="
        find / -name 'libcuda.so*' 2>/dev/null

        echo ""
        echo "=== /usr/local/cuda/compat (forward-compat libs) ==="
        ls -la /usr/local/cuda/compat/ 2>/dev/null || echo "(not found)"

        echo ""
        echo "=== torch CUDA version ==="
        python3 -c "import torch; print('torch CUDA runtime:', torch.version.cuda)"

        echo ""
        echo "=== torch.cuda.is_available ==="
        python3 -c "import torch; print('is_available:', torch.cuda.is_available())" 2>&1 || true
      resources:
        limits:
          nvidia.com/gpu: "1"
      env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: "all"
      - name: NVIDIA_DRIVER_CAPABILITIES
        value: "compute,utility"