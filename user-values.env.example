NAMESPACE=vllm-inference
HF_DIR=/models-cache
PVC_SIZE=40Gi
MODEL_ID="Qwen/Qwen2.5-7B-Instruct-AWQ"
RHIIS_IMAGE="registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.3.0" #actual as of late Feb 2026
HF_TOKEN=hf_<your_token>
