apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  namespace: ${NAMESPACE}
  labels:
    app: vllm
spec:
  replicas: 1
  # Recreate is required because the PVC is ReadWriteOnce — only one pod
  # can mount it at a time. Rolling update would cause the new pod to
  # get stuck waiting for the volume.
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      serviceAccountName: vllm-sa
      securityContext:
        runAsUser: 0

      # Kubernetes injects <SERVICE_NAME>_PORT=tcp://... env vars for every
      # Service in the namespace. Because the Service is named "vllm", this
      # produces VLLM_PORT=tcp://..., which collides with vLLM's own VLLM_PORT
      # variable (expects a plain integer port). Disabling service links
      # prevents all such injections.
      enableServiceLinks: false

      # Tolerate GPU taint so the pod is schedulable on GPU nodes.
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"

      containers:
        - name: server
          image: ${RHIIS_IMAGE}
          # Shell wrapper: download model on first start, skip on subsequent starts,
          # then exec into the vLLM server. exec replaces the shell so the Python
          # process receives SIGTERM directly for graceful shutdown.
          command:
            - /bin/sh
            - -c
            - |
              set -e
              MODEL_DIR=${MODEL_PATH}
              if [ ! -f "${MODEL_DIR}/config.json" ]; then
                echo "Downloading ${MODEL_ID} from Hugging Face Hub..."
                huggingface-cli download ${MODEL_ID} \
                  --local-dir "${MODEL_DIR}" \
                  --local-dir-use-symlinks False
                echo "Download complete."
              else
                echo "Model already present at ${MODEL_DIR}, skipping download."
              fi
              exec python -m vllm.entrypoints.openai.api_server \
                --model "${MODEL_DIR}" \
                --served-model-name "${MODEL_ID}" \
                --dtype auto \
                --tensor-parallel-size 1 \
                --max-num-seqs 128 \
                --max-model-len 4096 \
                --gpu-memory-utilization 0.9 \
                --enforce-eager \
                --port 8000
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
            - name: HF_HOME
              value: ${HF_DIR}/.hf-cache
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          resources:
            requests:
              cpu: "2"
              memory: "6Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: models-cache
              mountPath: ${HF_DIR}
            # Shared memory for PyTorch tensor operations across workers.
            - name: shm
              mountPath: /dev/shm
          # startupProbe allows up to 12 minutes (24 × 30s) for download +
          # model load before liveness/readiness checks begin.
          startupProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 30
            failureThreshold: 24
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 15
            failureThreshold: 3
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 30
            failureThreshold: 3
            timeoutSeconds: 5

      volumes:
        - name: models-cache
          persistentVolumeClaim:
            claimName: vllm-models-cache
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
